## redis端口 51827 容量满的问题

> redis信息统计
> 
> key总数 **5740万**
> 
> 
> 1. 视频集父子关系: **4864万,占比84%**
> 
> 2. 视频集名称和视频集ID的映射关系:**占比约10%**
> 
> 3. 初始化榜单数据: 极少
> 
> 4. union的一个配置,明星代投稿,用户白名单  极少
> 
> 抽样redis下行97%命令为MGET(根据视频集名称->视频集ID)

### 1. 视频集父子关系(主要影响因素)

- 业务场景:  视频集一级目录,二级目录 

- 数据结构: **zset**, **value**: 子视频集ID, **score**: 插入时间

- 过期时间: 无过期

下行`MediaClusterRpcService.getClusterIdsByParentIdAndType`调用量: **< 1 QPS**

接口逻辑: 根据以下参数,从Redis的zset中根据时间翻页获取

| 参数 | 说明 |
| ------ | ------ |
| parentClusterId | 顶级视频集ID |
| childClusterType | 子视频集类型 |
| count | 单页个数 |
| startTime | 开始时间 |
| endTime | 结束时间 |
| cursor | 翻页游标 |


问题总结: 

父子视频集缓存使用zset数据结构存储,无过期时间, 永久存储是否有必要? 加上缓存过期时间数据从哪里出来?代价是多少? 调用量是多少? 

1. 下行`MediaClusterRpcService.getClusterIdsByParentIdAndType` 24小时调用量: **164次**
2. 上行写入**QPS= 1.17**  (`info.log.20220119-21.gz` 一个小时调用zadd命令**4221次**,一天数量**63480**)
3. redis中的数据有效数据的比例大致是多少?  164/63480=0.2%

每天数据增量大小估算

#### 方案1: 
redis使用Pika替代

pika解决redis在容量过大出现

  1.dba迁移redis数据到pika
  2.dba将redis的数据实时同步到pika，确保redis与pika的数据始终一致
  3.dba切换lvs后端ip，由pika替换redis
  注：pika提供redis_to_pika工具，通过aof来将db和实时增量数据同步到pika


#### 方案2:

从MySQL中直接从现有数据获取

**痛点:**
`cluster_item` 表中没有子视频集的类型, 查不出来,需要查所有子视频集类型,进行筛选->塞入缓存

**写QPS: 1.17**

**读: 一天164次**

**从`cluster_item`中获取, 因为没有二级目录类型的筛选, 成本太大如下**

<img src=http://git.intra.weibo.com/im/form//uploads/6bbe5d66bdfea476048a519c288f2b39/image.png width=300 height=500 />

#### redis拆端口:

需要洗数据

1. 双写redis缓存, 下行读使用旧缓存
2. 洗数据,将原有redis数据按照hash的方式写入新缓存
3. 开关切读取新缓存
4. 停写旧缓存,下掉旧缓存代码





----


### 2. 视频集名称和视频集ID的映射关系(影响不大)
- QPS:  下行5W, 上行写redis未统计  7.5K
- 命中全局唯一的视频集类型

- 业务场景:  唯一视频集名称 ---> 该视频集ID, 以此过滤重复视频集名称.
- 数据结构: String
- 过期时间: 无过期

调用量: 等同于创建视频集名称唯一类型的写入量

### 3. 初始化榜单数据  查看无线上调用日志(忽略)

### 4. union_drools_list: 配置项,量很少(影响可忽略)

更新分发规则 明星代投稿 用户白名单
