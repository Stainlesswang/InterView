### 需求背景

原有设计一个用户下某类型的视频集有最大数量限制, 业务发展导致用户下视频集过大,出现MC缓存失效问题.

> 以**微博故事**创建 type=52(小视频热点)`short_video_hot` 的视频集为例, 目前该uid:**6190532921** 该类型的视频集达到了**12689个视频集**
> 
> 加上其他类型的视频集, 总数达到**2万2千+**,且缓存内容为视频集详情,单个**100-120字节为例**,该用户缓存内容原始大小**2.3M+**

> **获取该用户视频集的下行操作** 都会穿DB导致存在大量慢查询,达到 **10s-25s** DB压力过大



**线上临时解决方式:** 

`select * uid=? from user_cluster limit 2000`

**目前存在问题:** 

以**微博故事账号为例**

> 以微博故事为例,limit 2000 按照自增id升序查出数据

1. 新建自定义视频集,无法展示出来(因为限制2000里查出的都是旧数据)
2. 虽然操作新建的视频集类型是type=52,但是影响所有type的数据


现状时序图:

```plantuml
Storage -> UserPlayListCache: 缓存是否命中? 命中结束即可
Storage -> DBuser_cluster: 获取uid所有视频集id
DBuser_cluster -> Storage: uid所有视频集id
Storage-> DBcluster:获取所有视频集详情
DBcluster -> Storage: 视频集详情
Storage ->x UserPlayListCache: set操作失败
```

现状QPS:

1. 创建或者更新视频集`MediaClusterRpcService.createOrUpdateCluster ` **60 QPS**
2. 下行获取视频集信息`MediaClusterRpcService.getClusterIdsByUidsAndType`  **14KQPS**
3. 下行获取`MediaClusterRpcService.getClusterMediaVOSByClusterIdWithCursor ` **10KQPS**

### 方案

**缓存中缓存uid某type最新200条数据, 取200条之后的从DB获取**

#### 整体流程

<img src=uploads/797c12a152ccd815e179ba7dbb95f873/image.png width=500 height=500 />


#### 接口文档

- 接口类型: RPC
- 参数类型

|  参数   | 类型 |  备注 |
|  ----  | ----  | ---- |
| uid   | long | 用户uid |
| type  | int | 视频集类型 例如: 2:用户自定义视频集  |
| count | int | 本页获取的条数  默认20 |
| page  | int | 第几页  |
| total | int | 是否返回总数, 1:返回 0:不返回  默认0 |

备注:  用户总数梳理medialib各处,使用点只有创建用户自定义视频集的时候判断是否超过限制. 目前对count如果有需要的话,直接从DB查找即可


#### 类图

                                              
#### 缓存Cache层设计

##### 只缓存前200条

- 数据支撑, 随机抽取五张表进行统计 , uid下所有视频集总数

|  DB   | 0-100 |  100-200 | 200+  |
|  ----  | ----  | ---- | ---- |
| `user_cluster_1b7`  | 大部分 | 1 | 3|
| `user_cluster_0c5`  | 大部分 | 5 | 1
| `user_cluster_0c7`  | 大部分 | 2 | 0
| `user_cluster_007`  | 大部分 | 1 | 4
| `user_cluster_005`  | 大部分 | 3 | 0

**结论:**

大于200视频集的用户预估 700-1000人

200作为作为缓存数据能够保证 **99.99%以上**的场景


- 缓存结构

|  Key  | Value|
|  ----  | ----  | 
| `$uid_$type_plalistInfo`  | `List<PlaylistBean>` 视频集详情结合 |

- 缓存使用方式 **Read/Write Through（读写穿透）的变种**

>Write: Cache存不存在都更新DB,更新Cache  (Read/Write Through 方式Cache不存在只更新DB,下行刷缓存)
>
>Read: miss后加载到缓存

> 缓存miss塞入占位符(过期时间一个小时)
> 普通数据过期时间24小时


##### 缓存每页数据方案

假设 cacheSize=200, totoalNum=2000, 则需要缓存页数最大为10,则缓存序号为 1-10

> 如果200每页的缓存未命中, 则透DB查找数据, 将下一个200页缓存
> 
> 上行 **add 操作** 所有缓存页失效, 删除全部缓存页
> 
> 上行**update操作** 需计算缓存页序号为N, 则影响 序号=N的缓存页
> 
> 上行**delete操作** 需计算缓存序号为N, 将序号>=N的缓存全部删除
> 
>  下次请求到来, 根据用户参数以及**总数**判断在缓存的哪一部分? 尝试去获取该缓存页, 如果没有从DB中查找该缓存页入缓存, 有的话直接获取数据返回即可

上述方案流程复杂,维护缓存多页数据不一致性风险过大, 并且当一个用户频繁创建和删除的情况下,分页缓存全部失效,同样会存在穿透DB的情况, 一次上行操作,额外增加多次缓存判断有无, 缓存一致性维护的操作.

开发成本大,效果不佳,不建议采用


#### 历史获取代码修改

1. 获取uid某个type的全量, 只取缓存最大数量(200)的视频集详情
	
	风险点: 对外暴露的RPC接口为 
	1. `MediaClusterRpcService.getClusterIdsByUidsAndType` 根据uid+type获取所有视频集id
	2. `MediaClusterRpcService.getClusterInfosByUidsAndType ` 根据uid+type获取所有视频集详情
    
2. 批量获取多个uid的某一类type, 只取每个uid的缓存最大数量(200)的视频集详情. 
3.   medialib内部Service层的调用, 现在的使用方式都是获取全量的方式,修改要梳理每个的上游调用以及业务逻辑

	

#### 上线灰度流程

下文使用`uid_cache`代表以uid作为key的缓存, 使用`uid_type_cache`代表以uid+type作为key的缓存

* 第一步: 视频集add,update,delete的时候更新缓存, reload `uid_cache` && reload `uid_type_cache`
* 第二步: 下行增加控制是否启用新缓存,开关`open.user.playlist.new=false` 默认关闭,读旧缓存
* 第三部: 48小时后(缓存失效24小时,上线双写48小时)打开新缓存开关,使用`uid_type_cache`做下行


#### 关联优化点:

制定方案过程中发现一些需要修改的bug以及一些逻辑上的优化

1. bug修改 `MediaClusterRpcService`入口新建用户自定义视频集无上限限制, 目前线上逻辑上限200条
2. union项目中存在以`uid+type`作为key的缓存,采用本方案后,union可以下掉该缓存